{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635412e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required packages\n",
    "import pandas as pd\n",
    "import glob\n",
    "import duckdb\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "csv_files_dustmonitoring = sorted(glob.glob(r'c:\\users\\sieweweiss/OneDrive - KPMG/Documents/JADS/Level 1/Enexis/data/Dustmonitoring/*.csv'), reverse=True)\n",
    "\n",
    "df_original = pd.DataFrame()\n",
    "\n",
    "df_original = pd.concat([pd.read_csv(\n",
    "        csv_file, \n",
    "        sep=';', \n",
    "        header=[0,1,2], \n",
    "        index_col=[0,1], \n",
    "        decimal=',', \n",
    "        low_memory=False)\n",
    "        for csv_file in csv_files_dustmonitoring])\n",
    "\n",
    "df_original.dtypes\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original = pd.read_pickle('df_original.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stack = df_original\n",
    "df_stack.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stack = df_stack.stack(level=0)\n",
    "df_stack.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stacked = df_stack.reset_index(level=[0,1], names=['Time_UTC', 'Time_Local'])\n",
    "df_stacked.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stacked.columns = df_stacked.columns.droplevel(1)\n",
    "df_stacked.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rename = df_stacked.reset_index(names='Location')\n",
    "df_rename.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rename = df_rename.rename_axis(index='Location', columns=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_rename.reset_index(drop=True)\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"Time_UTC\", \"Time_Local\"]] = df[[\"Time_UTC\", \"Time_Local\"]].apply(pd.to_datetime)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf600827",
   "metadata": {},
   "source": [
    "## Data Exploration & Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total number of locations in the dataset where the PM is measured \n",
    "df['Location'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nonNA = df.notna().groupby(df['Location']).sum().drop('Location', axis=1).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "\n",
    "chart = alt.Chart(df_nonNA).mark_bar().encode(\n",
    "    alt.Y('PM10').title('Sum of non-NA PM10 count'),\n",
    "    x='Location')\n",
    "line = alt.Chart(df_nonNA).mark_rule(strokeDash=[10, 10]).encode(y=alt.datum(149040))\n",
    "\n",
    "\n",
    "chart+line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Location IDs ordered by the number of nonNaN\n",
    "df_nonNA = df.PM10.notna().groupby(df.Location).sum().reset_index().sort_values(by='PM10', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_na = df.PM10.isna().groupby(df.Location).sum().reset_index().sort_values(by='PM10', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pm = df[['NO2', 'PM1', 'PM2.5', 'PM10', 'UFP']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(df_pm.corr(), annot=True)\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (10,7)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"PM2.5 and PM10 are the dust measurements which are legislated by the EU and therefore the two we are most interested in. As seen above, there is a strong correlation between \n",
    "PM10 and PM2.5. Based on research by the RIVM if the PM10 meets the legislated maximum value then PM2.5 is in nearly all cases also under the maximum legislated amount. \n",
    "Therefore the focus of our project will be on PM10 and we will also drop PM2.5\"\"\"\n",
    "df = df.drop(labels=['NO2', 'PM1', 'UFP', 'PM2.5'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['PM10'] = df['PM10'].astype('float32')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"PM10\"].describe().apply(lambda x: format(x, 'f'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('Location')['PM10'].describe().sort_values(by='count', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 largest PM10 values in the dataset\n",
    "df.nlargest(n=10, columns=['PM10'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 smallest PM10 values in the dataset\n",
    "df.nsmallest(n=10, columns=['PM10'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9478ece7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rounded = df\n",
    "df_rounded['Time_Local'] = pd.to_datetime(df_rounded['Time_Local']).dt.round('10min')\n",
    "df_rounded['Time_UTC'] = pd.to_datetime(df_rounded['Time_UTC']).dt.round('10min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c4f934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify missing PM10 records\n",
    "missing_count_per_location = df_rounded.groupby('Location')['PM10'].apply(lambda x: x.isna().sum())\n",
    "\n",
    "# Group by 'Location' and calculate the total number of records\n",
    "location_stats = df_rounded.groupby('Location')['PM10'].agg(['count', 'size'])\n",
    "\n",
    "# Calculate the total number of recorded data points per location\n",
    "total_recorded_data_points = location_stats['count']\n",
    "\n",
    "# Add the missing count to the location_stats DataFrame\n",
    "location_stats['missing_count'] = missing_count_per_location\n",
    "\n",
    "# Calculate the percentage of missing data as a percentage of the total recorded values\n",
    "location_stats['percentage_missing'] = (location_stats['missing_count'] / location_stats['count']) * 100\n",
    "\n",
    "# Display the result\n",
    "print(location_stats[['count', 'missing_count', 'percentage_missing']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c10f8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rounded['Time_Local'] = pd.to_datetime(df_rounded['Time_Local'])\n",
    "\n",
    "# Sort the DataFrame by 'Location' and 'Time_Local'\n",
    "df_rounded.sort_values(['Location', 'Time_Local'], inplace=True)\n",
    "\n",
    "# Calculate time differences between consecutive rows for each group\n",
    "df_rounded['time_diff'] = df_rounded.groupby('Location')['Time_Local'].diff()\n",
    "\n",
    "# Identify consecutive missing PM10 records greater than or equal to 2 hours\n",
    "threshold_2_hours = pd.Timedelta(hours=2)\n",
    "missing_data_2_hours = df_rounded[df_rounded['time_diff'] >= threshold_2_hours]\n",
    "\n",
    "# Group by 'Location' and count consecutive missing records\n",
    "missing_counts_2_hours = missing_data_2_hours.groupby('Location')['time_diff'].count()\n",
    "\n",
    "# Get Locations with 2 hours or more consecutively missing PM10 records\n",
    "locations_missing_2_hours = missing_counts_2_hours[missing_counts_2_hours >= 12].index  # 12 hours = 720 minutes / 10 minutes per measurement\n",
    "\n",
    "print(\"Locations missing 2 hours or more of consecutive PM10 records:\", locations_missing_2_hours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5801fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the DataFrame by 'Location' and 'Time_Local'\n",
    "df_rounded.sort_values(['Location', 'Time_Local'], inplace=True)\n",
    "\n",
    "# Identify the locations to be excluded from interpolation\n",
    "exclude_locations = ['I11', 'I22', 'I28', 'I30', 'I45']\n",
    "\n",
    "# Create a mask to filter out rows for excluded locations\n",
    "mask_exclude = ~df_rounded['Location'].isin(exclude_locations)\n",
    "\n",
    "# Apply linear interpolation for all locations except the excluded ones\n",
    "df_rounded.loc[mask_exclude, 'PM10'] = df_rounded.loc[mask_exclude, 'PM10'].interpolate()\n",
    "\n",
    "# Drop rows where 'Location' is 'I11', 'I22', 'I28', 'I30', 'I45'\n",
    "locations_to_drop = ['I11', 'I22', 'I28', 'I30', 'I45']\n",
    "df_rounded = df_rounded[~df_rounded['Location'].isin(locations_to_drop)]\n",
    "\n",
    "# Drop the time_diff column\n",
    "df_rounded = df_rounded.drop('time_diff', axis=1)\n",
    "\n",
    "# Display the result\n",
    "print(df_rounded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import hourly weather data from KNMI\n",
    "df_weather = pd.read_csv('KNMI Uurwaarnemingen.csv', parse_dates=['YYYYMMDD'])\n",
    "df_weather.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine column names\n",
    "df_weather.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# H column contains which hour the weather conditions were measured however this is in int dtype, we need to transform it to hour format to add it to a new Datetime column which we can join to the PM10 dataset.\n",
    "df_weather['Hours'] = df_weather['H'].astype(str)+':00:00'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 24:00:00 with 00:00:00 for consistency\n",
    "df_weather['Hours'] = df_weather['Hours'].replace('24:00:00', '00:00:00')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Datetime column on which we can join the weather data and PM10 data\n",
    "df_weather['Datetime'] = pd.to_datetime(df_weather['YYYYMMDD'].astype(str) + df_weather['Hours'], format='%Y-%m-%d%H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8633e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we previously saw that the column names contained spaces before and after the strings, we want to remove these with strip()\n",
    "df_weather.columns = df_weather.columns.str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2826f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the table with the added columns\n",
    "df_weather.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab955f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the PM10 dataset\n",
    "df_rounded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0378d5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Align datetime column format\n",
    "df_rounded['Datetime'] = pd.to_datetime(df_rounded['Time_Local'].dt.to_period('H').astype(str) + ':00', format='%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb876b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the hourly weather data and the PM10 data\n",
    "df_merged = pd.merge(df_rounded, df_weather, left_on='Datetime', right_on='Datetime', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f353a320",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72355521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save this merged dataset to a parquet file so that it can be shared if needed and can be loaded directly without redoing the previous steps\n",
    "df_merged.to_parquet('df_merged.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aaf3237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the merged dataset from the parquet file\n",
    "df = pd.read_parquet('df_merged.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create additional columns to capture day of the week, weekday/weekend data and create sine and cosine columns to capture the cyclicality of the days of the week.\n",
    "df['DayOfWeek'] = df['Time_Local'].dt.dayofweek\n",
    "df['Weekday_Type'] = df['Time_Local'].dt.dayofweek // 5 == 1\n",
    "df['Weekday_Type'] = df['Weekday_Type'].map({True: 1, False: 0})\n",
    "df['DayOfWeekSin'] = np.sin(2 * np.pi * df['DayOfWeek'] / 7)\n",
    "df['DayOfWeekCos'] = np.cos(2 * np.pi * df['DayOfWeek'] / 7)\n",
    "df['Month'] = df['Time_Local'].dt.month\n",
    "df['MonthSin'] = np.sin(2 * np.pi * df['Month'] / 12)\n",
    "df['MonthCos'] = np.cos(2 * np.pi * df['Month'] / 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a0c55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load daily weather data\n",
    "df_weather_daily = pd.read_csv(\"KNMI Dagwaarden.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbbdf74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strip the spaces before/after strings in the column names\n",
    "df_weather_daily.columns = df_weather_daily.columns.str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775fae92",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weather_daily.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72dbefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert YYYYMMDD column to datetime format\n",
    "df_weather_daily['YYYYMMDD'] = pd.to_datetime(df_weather_daily['YYYYMMDD'], format='%Y%m%d')\n",
    "\n",
    "# add '_d' to all columns to distinguish between the hourly weather features already in the datatset and the hourly features we are adding next\n",
    "df_weather_daily.columns = [col + '_d' for col in df_weather_daily.columns]\n",
    "\n",
    "# Create Date column in the df to join with df_weather_daily\n",
    "df['Date'] = pd.to_datetime(df['Time_Local'].dt.date, format='%Y%m%d')\n",
    "\n",
    "# Join the two DataFrames on 'YYYYMMDD' and 'Time_Local'\n",
    "result_df = pd.merge(df, df_weather_daily, left_on='Date', right_on='YYYYMMDD_d', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886e8353",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4bb507f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary columns\n",
    "columns_to_drop = ['Hours', 'Datetime', 'YYYYMMDD', 'YYYYMMDD_d', 'Date']\n",
    "result_df = result_df.drop(columns=columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d220782b",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df['Hour'] = result_df['Time_Local'].dt.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c1ece3",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df['hour_sin'] = np.sin(2 * np.pi * result_df['Hour'] / 24)\n",
    "result_df['hour_cos'] = np.cos(2 * np.pi * result_df['Hour'] / 24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bce837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scope for this project is only locations in Eindhoven, these location ID's are saved in a variable\n",
    "Eindhoven_locations = ['I02', 'I25', 'I14', 'I23', 'I04', 'I32', 'I37', 'I07', 'I22', 'I08', 'I28', 'I40', 'I30', 'I39', 'I12', 'I24', 'I11', 'I09', 'I36', 'I29', 'I19', 'I17']\n",
    "df_eindhoven = result_df[result_df['Location'].isin(Eindhoven_locations)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a45df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eindhoven.to_parquet('Dataset_Eindhoven.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de42648",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('Dataset_Eindhoven.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85828f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the dataframe by 'Location' and 'Time_UTC' in ascending order\n",
    "df = df.sort_values(by=['Location', 'Time_UTC'])\n",
    "\n",
    "# Forward fill missing values in 'Lat' and 'Lon' columns within each location\n",
    "df[['Lat', 'Lon']] = df.groupby('Location')[['Lat', 'Lon']].transform(lambda x: x.ffill())\n",
    "\n",
    "# If there are still remaining null values after forward filling, you can fill them with the last available value\n",
    "df[['Lat', 'Lon']] = df.groupby('Location')[['Lat', 'Lon']].transform(lambda x: x.fillna(method='bfill').fillna(method='ffill'))\n",
    "\n",
    "# Check if there are any remaining null values\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fdfcb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Sort df by Time_Local\n",
    "df = df.sort_values(by='Time_Local')\n",
    "\n",
    "# Specify the percentage of data for training (e.g., 70% for training, 30% for testing)\n",
    "train_size = 0.7\n",
    "\n",
    "# Calculate the index to split the data\n",
    "split_index = int(train_size * len(df))\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_data = df.iloc[:split_index, :]\n",
    "test_data = df.iloc[split_index:, :]\n",
    "\n",
    "# Separate features and target variable for training set\n",
    "X_train = train_data.drop(columns=['PM10', 'Time_UTC', 'Time_Local', 'Location', 'T10N', 'N', 'WW']) # Drop 'PM10' column as it is the target variable, Datetime and ID columns cant be part of X dataset and T10N, N and WW are missing too much data to include\n",
    "y_train = train_data['PM10']\n",
    "\n",
    "# Separate features and target variable for testing set\n",
    "X_test = test_data.drop(columns=['PM10', 'Time_UTC', 'Time_Local', 'Location', 'T10N', 'N', 'WW']) # Drop 'PM10' column as it is the target variable, Datetime and ID columns cant be part of X dataset and T10N, N and WW are missing too much data to include\n",
    "y_test = test_data['PM10']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9cfec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Create a Random Forest Regressor\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "rf_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda64415",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_rf = rf_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1210bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Evaluate the performance\n",
    "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
    "mae_rf = mean_absolute_error(y_test, y_pred_rf)\n",
    "r2_rf = r2_score(y_test, y_pred_rf)\n",
    "rmse_rf = np.sqrt(mse_rf)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(f'Mean Squared Error (Random Forest): {mse_rf:.2f}')\n",
    "print(f'Mean Absolute Error (Random Forest): {mae_rf:.2f}')\n",
    "print(f'R-squared (Random Forest): {r2_rf:.2f}')\n",
    "print(f'Root Mean Squared Error (Random Forest): {rmse_rf:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2e9bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "\n",
    "# Create an Extra Trees Regressor model\n",
    "extra_trees_model = ExtraTreesRegressor(random_state=42)\n",
    "\n",
    "# Train the model\n",
    "extra_trees_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_extra_trees = extra_trees_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model performance\n",
    "mse_extra_trees = mean_squared_error(y_test, y_pred_extra_trees)\n",
    "mae_extra_trees = mean_absolute_error(y_test, y_pred_extra_trees)\n",
    "r2_extra_trees = r2_score(y_test, y_pred_extra_trees)\n",
    "rmse_extra_trees = np.sqrt(mse_extra_trees)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(f'Mean Squared Error (Extra Trees): {mse_extra_trees:.2f}')\n",
    "print(f'Mean Absolute Error (Extra Trees): {mae_extra_trees:.2f}')\n",
    "print(f'R-squared (Extra Trees): {r2_extra_trees:.2f}')\n",
    "print(f'Root Mean Squared Error (Extra Trees): {rmse_extra_trees:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6794ebe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Create a Decision Tree Regressor model\n",
    "decision_tree_model = DecisionTreeRegressor(random_state=42)\n",
    "\n",
    "# Train the model\n",
    "decision_tree_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_decision_tree = decision_tree_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model performance\n",
    "mse_decision_tree = mean_squared_error(y_test, y_pred_decision_tree)\n",
    "mae_decision_tree = mean_absolute_error(y_test, y_pred_decision_tree)\n",
    "r2_decision_tree = r2_score(y_test, y_pred_decision_tree)\n",
    "rmse_decision_tree = np.sqrt(mse_decision_tree)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(f'Mean Squared Error (Decision Tree): {mse_decision_tree:.2f}')\n",
    "print(f'Mean Absolute Error (Decision Tree): {mae_decision_tree:.2f}')\n",
    "print(f'R-squared (Decision Tree): {r2_decision_tree:.2f}')\n",
    "print(f'Root Mean Squared Error (Decision Tree): {rmse_decision_tree:.2f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
